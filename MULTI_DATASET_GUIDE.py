"""
Multi-Dataset Training Guide
COMP64301: Computer Vision Coursework

Complete guide for training on CIFAR-10 and Caltech-101
"""

MULTI_DATASET_GUIDE = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              MULTI-DATASET CNN TRAINING GUIDE (2 DATASETS)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your project now supports TWO datasets for comprehensive evaluation!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š DATASET STRATEGY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATASET 1: CIFAR-10 (Primary)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Classes: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)
â€¢ Images: 60,000 (50k train, 10k test)
â€¢ Resolution: 32Ã—32 RGB
â€¢ Size: ~170 MB download
â€¢ Characteristics:
  âœ“ Low resolution - tests model on small images
  âœ“ Balanced classes - 6000 images per class
  âœ“ Fast to train - good for experimentation
  âœ“ Standard benchmark - easy to compare with literature

DATASET 2: Caltech-101 (Secondary)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Classes: 101 (various object categories)
â€¢ Images: ~9,000 total
â€¢ Resolution: Variable (~300Ã—200 average, resized to 224Ã—224)
â€¢ Size: ~130 MB download
â€¢ Characteristics:
  âœ“ High resolution - tests model on realistic images
  âœ“ More classes - tests model capacity
  âœ“ Imbalanced - 40-800 images per class (realistic scenario)
  âœ“ Variable sizes - tests preprocessing robustness

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ WHY THIS COMBINATION?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. DIFFERENT SCALES:
   â€¢ CIFAR-10: Small images (32Ã—32)
   â€¢ Caltech-101: Large images (224Ã—224)
   â†’ Shows your models work across different resolutions

2. DIFFERENT COMPLEXITY:
   â€¢ CIFAR-10: 10 classes (simpler)
   â€¢ Caltech-101: 101 classes (more complex)
   â†’ Tests model capacity and generalization

3. DIFFERENT DISTRIBUTIONS:
   â€¢ CIFAR-10: Balanced, uniform distribution
   â€¢ Caltech-101: Imbalanced, realistic distribution
   â†’ Shows robustness to data characteristics

4. FAIR COMPARISON:
   â€¢ Both CNN and Traditional CV will use BOTH datasets
   â€¢ This gives you 4 total models to compare:
     - CNN on CIFAR-10
     - CNN on Caltech-101
     - Traditional CV on CIFAR-10
     - Traditional CV on Caltech-101

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ QUICK START - TRAIN ON BOTH DATASETS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPTION 1: Train Both Datasets in One Go (Recommended)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    python train_all_datasets.py

This will:
âœ“ Train on CIFAR-10 first
âœ“ Then train on Caltech-101
âœ“ Save results for both
âœ“ Generate comparison summary


OPTION 2: Train Each Dataset Separately
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For CIFAR-10:
    python main_cnn.py

For Caltech-101:
    python main_cnn_caltech101.py


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â±ï¸  TRAINING TIME ESTIMATES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CIFAR-10 (50 epochs):
  â€¢ CPU: 20-30 minutes
  â€¢ GPU: 5-10 minutes

Caltech-101 (50 epochs):
  â€¢ CPU: 30-50 minutes (larger images)
  â€¢ GPU: 10-15 minutes

TOTAL for both: 1-2 hours on CPU, 15-25 minutes on GPU

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ EXPECTED PERFORMANCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CIFAR-10:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ Test Acc     â”‚ Parameters â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BaselineCNN â”‚ 75-80%       â”‚ ~1.2M      â”‚
â”‚ ImprovedCNN â”‚ 80-85%       â”‚ ~1.8M      â”‚
â”‚ ResNet18    â”‚ 85-90%       â”‚ ~11M       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caltech-101:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ Test Acc     â”‚ Parameters â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BaselineCNN â”‚ 50-60%       â”‚ ~1.2M      â”‚
â”‚ ImprovedCNN â”‚ 60-70%       â”‚ ~1.8M      â”‚
â”‚ ResNet18    â”‚ 70-80%       â”‚ ~11M       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Caltech-101 is harder (more classes, less data per class)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ CUSTOMIZATION FOR EACH DATASET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

configs/config.py contains separate configurations:

CIFAR10Config:
  â€¢ INPUT_SIZE = (32, 32)
  â€¢ NUM_CLASSES = 10
  â€¢ BATCH_SIZE = 32
  â€¢ Use native resolution

Caltech101Config:
  â€¢ INPUT_SIZE = (224, 224)
  â€¢ NUM_CLASSES = 101
  â€¢ BATCH_SIZE = 32
  â€¢ Higher resolution for better feature extraction

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ FOR YOUR COURSEWORK REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Having 2 datasets allows you to discuss:

1. GENERALIZATION:
   â€¢ How well do models transfer across datasets?
   â€¢ Which architecture is most robust?

2. SCALABILITY:
   â€¢ How does performance change with more classes?
   â€¢ Does the model handle variable resolution?

3. DATA EFFICIENCY:
   â€¢ CIFAR-10 has more samples per class
   â€¢ Caltech-101 has fewer samples per class
   â€¢ How does this affect learning?

4. COMPARISON DEPTH:
   â€¢ Compare CNN vs Traditional CV on BOTH datasets
   â€¢ Which approach works better on each?
   â€¢ Why might one be better for certain characteristics?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ COURSEWORK STRUCTURE WITH 2 DATASETS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Part 1-2: CNN Approach (16 marks)
  â”œâ”€â”€ CIFAR-10 experiments
  â”‚   â”œâ”€â”€ Baseline model
  â”‚   â”œâ”€â”€ Hyperparameter tuning
  â”‚   â””â”€â”€ Results analysis
  â”‚
  â””â”€â”€ Caltech-101 experiments
      â”œâ”€â”€ Same architecture
      â”œâ”€â”€ Adapted hyperparameters
      â””â”€â”€ Comparative analysis

Part 3-4: Traditional CV Approach (16 marks)
  â”œâ”€â”€ CIFAR-10 experiments
  â”‚   â”œâ”€â”€ SIFT/ORB features
  â”‚   â”œâ”€â”€ Bag-of-Words
  â”‚   â””â”€â”€ SVM/KNN classification
  â”‚
  â””â”€â”€ Caltech-101 experiments
      â”œâ”€â”€ Same pipeline
      â”œâ”€â”€ Parameter tuning
      â””â”€â”€ Comparative analysis

Part 5: Comparison (3 marks)
  â””â”€â”€ CNN vs Traditional CV on BOTH datasets
      â”œâ”€â”€ Which works better where?
      â”œâ”€â”€ Why?
      â””â”€â”€ Trade-offs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ PRO TIPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. START WITH CIFAR-10:
   âœ“ Faster to train
   âœ“ Good for debugging
   âœ“ Test hyperparameters here first

2. USE INSIGHTS FROM CIFAR-10 FOR CALTECH-101:
   âœ“ Best hyperparameters from CIFAR-10
   âœ“ Apply to Caltech-101 with minor adjustments
   âœ“ Document what transfers and what doesn't

3. MODEL SELECTION:
   âœ“ Baseline: Good for understanding fundamentals
   âœ“ Improved: Better balance of performance/complexity
   âœ“ ResNet18: Best performance, use for final results

4. FOR CALTECH-101:
   âœ“ Use higher learning rate (more classes to learn)
   âœ“ More dropout (prevent overfitting on small dataset)
   âœ“ More augmentation (limited data per class)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š RESULTS ORGANIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

results/
â”œâ”€â”€ cnn/
â”‚   â”œâ”€â”€ baseline_cifar10_*/          # CIFAR-10 experiments
â”‚   â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”‚   â”œâ”€â”€ training_history.json
â”‚   â”‚   â””â”€â”€ final_results.json
â”‚   â”‚
â”‚   â”œâ”€â”€ improved_caltech101_*/       # Caltech-101 experiments
â”‚   â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”‚   â”œâ”€â”€ training_history.json
â”‚   â”‚   â””â”€â”€ final_results.json
â”‚   â”‚
â”‚   â””â”€â”€ multi_dataset_summary.json   # Overall comparison
â”‚
â””â”€â”€ comparison/                       # Cross-dataset analysis
    â””â”€â”€ (will be created later)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… COMPLETE WORKFLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WEEK 1: CNN Implementation
  Day 1-2: Train on CIFAR-10
    â–¡ python main_cnn.py
    â–¡ python hyperparameter_tuning.py
  
  Day 3-4: Train on Caltech-101
    â–¡ python main_cnn_caltech101.py
    â–¡ Adapt hyperparameters
  
  Day 5: Compare results
    â–¡ Generate visualizations
    â–¡ Create comparison tables

WEEK 2: Traditional CV Implementation
  Day 1-2: Implement on CIFAR-10
    â–¡ SIFT/ORB extraction
    â–¡ Bag-of-Words
    â–¡ Classification
  
  Day 3-4: Implement on Caltech-101
    â–¡ Same pipeline
    â–¡ Parameter tuning
  
  Day 5: Compare CNN vs Traditional CV

WEEK 3: Report Writing
  â–¡ Write methods sections
  â–¡ Create figures and tables
  â–¡ Analysis and discussion
  â–¡ State-of-the-art review

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ SUCCESS CRITERIA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Train CNN on BOTH datasets
âœ“ Implement Traditional CV on BOTH datasets
âœ“ Fair comparison with same random seeds
âœ“ Document differences in approach for each dataset
âœ“ Analyze why certain methods work better
âœ“ Generate high-quality visualizations
âœ“ Write clear, justified report

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Good luck! You now have a comprehensive framework for multi-dataset evaluation! ğŸš€

Questions? Check:
  â€¢ README.md for project overview
  â€¢ CNN_USAGE_GUIDE.py for detailed CNN instructions
  â€¢ Discussion forum on Canvas
"""

if __name__ == "__main__":
    print(MULTI_DATASET_GUIDE)
