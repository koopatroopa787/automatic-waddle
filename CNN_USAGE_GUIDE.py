"""
CNN Implementation Usage Guide
COMP64301: Computer Vision Coursework

This guide explains how to use the CNN implementation for your coursework.
"""

USAGE_GUIDE = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   CNN IMPLEMENTATION USAGE GUIDE                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The CNN implementation is complete! Here's how to use it for your coursework.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¦ STEP 1: INSTALL DEPENDENCIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Navigate to the project directory and install requirements:

    cd cv_coursework
    pip install -r requirements.txt

This will install:
    â€¢ PyTorch and torchvision (deep learning)
    â€¢ OpenCV (computer vision)
    â€¢ scikit-learn (traditional ML)
    â€¢ matplotlib, seaborn (visualization)
    â€¢ And other necessary packages

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ STEP 2: TRAIN BASELINE CNN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Run the main training script:

    python main_cnn.py

This will:
    âœ“ Download CIFAR-10 dataset (~170 MB)
    âœ“ Create train/validation/test splits
    âœ“ Train a baseline CNN model
    âœ“ Save the best model
    âœ“ Generate training metrics
    âœ“ Test the final model

Expected output:
    â€¢ Best model saved to: models/cnn/best_model.pth
    â€¢ Training history: results/cnn/training_history.json
    â€¢ Final results: results/cnn/final_results.json

Training time: ~15-30 minutes on CPU, ~5-10 minutes on GPU

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ STEP 3: HYPERPARAMETER TUNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Run systematic hyperparameter exploration:

    python hyperparameter_tuning.py

This script will test different configurations:
    â€¢ Learning rates: [0.001, 0.01, 0.1]
    â€¢ Batch sizes: [32, 64, 128]
    â€¢ Weight decay: [1e-4, 1e-3, 1e-2]
    â€¢ Dropout rates: [0.3, 0.5, 0.7]

You can modify the param_grid in hyperparameter_tuning.py to test:
    â€¢ Different architectures ('baseline', 'improved', 'vgg', 'resnet18')
    â€¢ Optimizer parameters
    â€¢ Data augmentation settings
    â€¢ Input image sizes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š STEP 4: VISUALIZE RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Create visualizations for your report:

    from src.visualization.plots import (
        plot_training_curves,
        plot_hyperparameter_comparison,
        visualize_experiment_results
    )
    
    # Visualize a single experiment
    visualize_experiment_results('results/cnn/baseline_cnn_cifar10_20241128_120000')
    
    # Compare multiple experiments
    results = {
        'LR=0.001': history1,
        'LR=0.01': history2,
        'LR=0.1': history3
    }
    plot_hyperparameter_comparison(results, metric='val_acc')

This generates:
    â€¢ Training/validation curves
    â€¢ Learning rate schedules
    â€¢ Comparison plots
    â€¢ Confusion matrices

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¨ CUSTOMIZATION OPTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. CHANGE MODEL ARCHITECTURE:
   
   Edit main_cnn.py:
   
   model = create_model(
       model_name='improved',  # Try: 'baseline', 'improved', 'vgg', 'resnet18'
       num_classes=10,
       dropout_rate=0.5
   )

2. MODIFY DATA AUGMENTATION:
   
   Edit configs/config.py:
   
   AUGMENTATION_PARAMS = {
       'horizontal_flip': True,
       'vertical_flip': False,
       'rotation_range': 15,
       'zoom_range': 0.1,
   }

3. ADJUST TRAINING SETTINGS:
   
   Edit configs/config.py:
   
   EPOCHS = 50
   LEARNING_RATE = 0.001
   BATCH_SIZE = 64
   WEIGHT_DECAY = 1e-4

4. USE DIFFERENT INPUT SIZE:
   
   data_loader = create_cifar10_loaders(
       input_size=64  # Upscale from 32x32 to 64x64
   )

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ FOR YOUR REPORT (Parts 1-2)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT TO INCLUDE:

1. METHODS SECTION (Part 1: 8 marks):
   â€¢ Network architecture description
   â€¢ Justification for design choices
   â€¢ Hyperparameters tested and why
   â€¢ Data preprocessing steps
   â€¢ Training procedure
   
   Use: print_model_summary(model) to get architecture details

2. RESULTS SECTION (Part 2: 8 marks):
   â€¢ Training curves (loss and accuracy)
   â€¢ Hyperparameter exploration results
   â€¢ Best configuration found
   â€¢ Test set performance
   â€¢ Comparison tables
   
   Use: plot_training_curves() and plot_hyperparameter_comparison()

TIPS:
   âœ“ Focus on UNDERSTANDING, not just results
   âœ“ Explain WHY you chose certain hyperparameters
   âœ“ Discuss what worked and what didn't
   âœ“ Use appropriate figures and tables
   âœ“ Reference figures in your text

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ QUICK EXPERIMENTS TO TRY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. BASELINE COMPARISON:
   â€¢ Train baseline, improved, vgg, and resnet18
   â€¢ Compare performance and parameters
   â€¢ Analyze trade-offs

2. LEARNING RATE STUDY:
   â€¢ Test: [0.0001, 0.001, 0.01, 0.1]
   â€¢ Plot validation accuracy vs learning rate
   â€¢ Find optimal value

3. DROPOUT EFFECT:
   â€¢ Test: [0.0, 0.3, 0.5, 0.7]
   â€¢ Compare train vs validation accuracy
   â€¢ Analyze overfitting

4. BATCH SIZE IMPACT:
   â€¢ Test: [16, 32, 64, 128]
   â€¢ Compare training time and accuracy
   â€¢ Discuss memory vs performance

5. DATA AUGMENTATION:
   â€¢ Train with and without augmentation
   â€¢ Compare generalization
   â€¢ Show example augmented images

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ› TROUBLESHOOTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ISSUE: Out of memory error
FIX: Reduce batch size in configs/config.py

ISSUE: Training too slow
FIX: Use GPU if available, or reduce epochs/model size

ISSUE: Poor accuracy
FIX: Check data loading, try different learning rates, add augmentation

ISSUE: Model overfitting
FIX: Increase dropout, add weight decay, use data augmentation

ISSUE: Import errors
FIX: Make sure you're in the project root directory when running scripts

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ UNDERSTANDING THE OUTPUT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

After training, you'll find:

results/cnn/
â”œâ”€â”€ baseline_cnn_cifar10_20241128_120000/
â”‚   â”œâ”€â”€ best_model.pth              # Best model weights
â”‚   â”œâ”€â”€ training_history.json       # Loss/accuracy per epoch
â”‚   â”œâ”€â”€ final_results.json          # Summary statistics
â”‚   â””â”€â”€ figures/                    # Generated plots
â”‚       â”œâ”€â”€ training_curves.png
â”‚       â””â”€â”€ learning_rate.png

models/cnn/
â””â”€â”€ best_model.pth                  # Copy of best model

Use these files for:
    â€¢ Loading trained models
    â€¢ Creating report figures
    â€¢ Analyzing hyperparameter effects
    â€¢ Comparing different experiments

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… CHECKLIST FOR COURSEWORK PARTS 1-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Part 1: CNN Methods (8 marks)
â–¡ Train baseline CNN on CIFAR-10
â–¡ Document network architecture
â–¡ Test at least 3 hyperparameters
â–¡ Justify all design choices
â–¡ Describe data preprocessing
â–¡ Explain training procedure

Part 2: CNN Results (8 marks)
â–¡ Create training/validation curves
â–¡ Present hyperparameter results
â–¡ Include comparison tables/charts
â–¡ Interpret the results
â–¡ Discuss what worked/didn't work
â–¡ Suggest future improvements

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ NEXT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Run main_cnn.py to train baseline model
2. Run hyperparameter_tuning.py for systematic exploration
3. Create visualizations for your report
4. Document your findings
5. Move on to Traditional CV implementation (Parts 3-4)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Good luck with your coursework! ğŸš€

For questions, refer to:
    â€¢ README.md for project overview
    â€¢ Discussion forum on Canvas
    â€¢ TAs: George Bird, Kai Cao
"""

if __name__ == "__main__":
    print(USAGE_GUIDE)
